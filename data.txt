[Document(metadata={'Published': '2020-09-23', 'Title': 'FastSecAgg: Scalable Secure Aggregation for Privacy-Preserving Federated Learning', 'Authors': 'Swanand Kadhe, Nived Rajaraman, O. Ozan Koyluoglu, Kannan Ramchandran', 'Summary': "Recent attacks on federated learning demonstrate that keeping the training data on clients' devices does not provide sufficient privacy, as the model parameters shared by clients can leak information about their training data. A 'secure aggregation' protocol enables the server to aggregate clients' models in a privacy-preserving manner. However, existing secure aggregation protocols incur high computation/communication costs, especially when the number of model parameters is larger than the number of clients participating in an iteration -- a typical scenario in federated learning.\n  In this paper, we propose a secure aggregation protocol, FastSecAgg, that is efficient in terms of computation and communication, and robust to client dropouts. The main building block of FastSecAgg is a novel multi-secret sharing scheme, FastShare, based on the Fast Fourier Transform (FFT), which may be of independent interest. FastShare is information-theoretically secure, and achieves a trade-off between the number of secrets, privacy threshold, and dropout tolerance. Riding on the capabilities of FastShare, we prove that FastSecAgg is (i) secure against the server colluding with 'any' subset of some constant fraction (e.g. $\\sim10\\%$) of the clients in the honest-but-curious setting; and (ii) tolerates dropouts of a 'random' subset of some constant fraction (e.g. $\\sim10\\%$) of the clients. FastSecAgg achieves significantly smaller computation cost than existing schemes while achieving the same (orderwise) communication cost. In addition, it guarantees security against adaptive adversaries, which can perform client corruptions dynamically during the execution of the protocol."}, page_content='FastSecAgg: Scalable Secure Aggregation for\nPrivacy-Preserving Federated Learning\nSwanand Kadhe, Nived Rajaraman, O. Ozan Koyluoglu, and Kannan Ramchandran\nDepartment of Electrical Engineering and Computer Sciences\nUniversity of California Berkeley\n{swanand.kadhe,nived,ozan.koyluoglu,kannanr}@berkeley.edu\nABSTRACT\nRecent atacks on federated learning demonstrate that keeping the\ntraining data on clients’ devices does not provide suﬃcient privacy,\nas the model parameters shared by clients can leak information\nabout their training data. A secure aggregation protocol enables\nthe server to aggregate clients’ models in a privacy-preserving\nmanner. However, existing secure aggregation protocols incur high\ncomputation/communication costs, especially when the number of\nmodel parameters is larger than the number of clients participating\nin an iteration – a typical scenario in federated learning.\nIn this paper, we propose a secure aggregation protocol, Fast-\nSecAgg, that is eﬃcient in terms of computation and communi-\ncation, and robust to client dropouts. Te main building block of\nFastSecAgg is a novel multi-secret sharing scheme, FastShare,\nbased on the Fast Fourier Transform (FFT), which may be of inde-\npendent interest. FastShare is information-theoretically secure,\nand achieves a trade-oﬀbetween the number of secrets, privacy\nthreshold, and dropout tolerance. Riding on the capabilities of\nFastShare, we prove that FastSecAgg is (i) secure against the\nserver colluding with any subset of some constant fraction (e.g.\n∼10%) of the clients in the honest-but-curious seting; and (ii) tol-\nerates dropouts of a random subset of some constant fraction (e.g.\n∼10%) of the clients. FastSecAgg achieves signiﬁcantly smaller\ncomputation cost than existing schemes while achieving the same\n(orderwise) communication cost. In addition, it guarantees security\nagainst adaptive adversaries, which can perform client corruptions\ndynamically during the execution of the protocol.\nCCS CONCEPTS\n•Security and privacy →Privacy-preserving protocols;\nKEYWORDS\nsecret sharing, secure aggregation, federated learning, machine\nlearning, data sketching, privacy-preserving protocols\n1\nINTRODUCTION\nFederated Learning (FL) is a distributed learning paradigm that\nenables a large number of clients to coordinate with a central server\nto learn a shared model while keeping all the training data on clients’\ndevices. In particular, training a neural net in a federated manner\nis an iterative process. In each iteration of FL, the server selects a\nA shorter version of this paper has been accepted in ICML Workshop on Federated\nLearning for User Privacy and Data Conﬁdentiality, July 2020, and CCS Workshop on\nPrivacy-Preserving Machine Learning in Practice, November 2020.\nsubset of clients, and sends them the current global model. Each\nclient runs several steps of mini-batch stochastic gradient descent,\nand communicates its model update—the diﬀerence between the\nlocal model and the received global model—to the server. Te server\naggregates the model updates from the clients to obtain an improved\nglobal model, and moves to the next iteration.\nFL ensures that every client keeps their local data on their own\ndevice, and sends only model updates that are functions of their\ndataset. Even though any update cannot contain more information\nthan the client’s local dataset, the updates may still leak signiﬁcant\ninformation making them vulnerable to inference and inversion\natacks [17, 19, 33, 40]. Even well-generalized deep models such as\nDenseNet can leak a signiﬁcant amount of information about their\ntraining data [33]. In fact, certain neural networks (e.g., generative\ntext models) trained on sensitive data (e.g., private text messages)\ncan memorize the training data [13].\nSecure aggregation protocols can be used to provide strong pri-\nvacy guarantees in FL. At a high level, secure aggregation is a secure\nmulti-party computation (MPC) protocol that enables the server\nto compute the sum of clients'), Document(metadata={'Published': '2020-07-12', 'Title': 'VAFL: a Method of Vertical Asynchronous Federated Learning', 'Authors': 'Tianyi Chen, Xiao Jin, Yuejiao Sun, Wotao Yin', 'Summary': 'Horizontal Federated learning (FL) handles multi-client data that share the same set of features, and vertical FL trains a better predictor that combine all the features from different clients. This paper targets solving vertical FL in an asynchronous fashion, and develops a simple FL method. The new method allows each client to run stochastic gradient algorithms without coordination with other clients, so it is suitable for intermittent connectivity of clients. This method further uses a new technique of perturbed local embedding to ensure data privacy and improve communication efficiency. Theoretically, we present the convergence rate and privacy level of our method for strongly convex, nonconvex and even nonsmooth objectives separately. Empirically, we apply our method to FL on various image and healthcare datasets. The results compare favorably to centralized and synchronous FL methods.'}, page_content='VAFL: a Method of Vertical Asynchronous Federated Learning\nTianyi Chen 1 Xiao Jin 1 Yuejiao Sun 2 Wotao Yin 3\nAbstract\nHorizontal Federated learning (FL) handles multi-\nclient data that share the same set of features, and\nvertical FL trains a better predictor that combine\nall the features from different clients. This paper\ntargets solving vertical FL in an asynchronous\nfashion, and develops a simple FL method. The\nnew method allows each client to run stochas-\ntic gradient algorithms without coordination with\nother clients, so it is suitable for intermittent con-\nnectivity of clients. This method further uses a\nnew technique of perturbed local embedding to\nensure data privacy and improve communication\nefﬁciency. Theoretically, we present the conver-\ngence rate and privacy level of our method for\nstrongly convex, nonconvex and even nonsmooth\nobjectives separately. Empirically, we apply our\nmethod to FL on various image and healthcare\ndatasets. The results compare favorably to cen-\ntralized and synchronous FL methods.\n1. Introduction\nFederated learning (FL) is an emerging machine learn-\ning framework where a central server and multiple clients\n(e.g., ﬁnancial organizations) collaboratively train a ma-\nchine learning model [19; 24; 4]. Compared with existing\ndistributed learning paradigms, FL raises new challenges\nincluding the difﬁculty of synchronizing clients, the hetero-\ngeneity of data, and the privacy of both data and models.\nMost of existing FL methods consider the scenario where\neach client has data of a different set of subjects but their\ndata share many common features. Therefore, they can\ncollaboratively learn a joint mapping from the feature space\nto the label space. This setting is also referred to data-\n1Department of Electrical, Computer, and Systems Engineering,\nRensselaer Polytechnic Institute, Troy, NY, USA. 2Department of\nMathematics, University of California, Los Angeles, Los Angeles,\nCA, USA. 3DAMO Academy, Alibaba US, Seattle, WA, USA.\nAuthors are listed in alphabetical order. Correspondence to: Wotao\nYin <wotao.yin@alibaba-inc.com>.\nProceedings of the ICML Workshop on Federated Learning for\nUser Privacy and Data Conﬁdentiality, July, 2020.\npartitioned or horizontal FL [20; 24].\nUnlike the data-partitioned setting, in many learning sce-\nnarios, multiple clients handle data about the same set of\nsubjects, but each client has a unique set of features. This\ncase arises in e-commerce, ﬁnancial, and healthcare applica-\ntions [13]. For example, an e-commerce company may want\nto predict a customer’s credit using her/his historical transac-\ntions from multiple ﬁnancial institutions; and, a healthcare\ncompany wants to evaluate the health condition of a particu-\nlar patient using his/her clinical data from various hospitals\n[36]. In these examples, data owners (e.g., ﬁnancial insti-\ntutions and hospitals) have different records of those users\nin their joint user base, so by combining their features, they\ncan establish a more accurate model. We refer to this setting\nas feature-partitioned or vertical FL [42].\nCompared to the relatively well-studied horizontal FL set-\nting [25], the vertical FL setting has its unique features and\nchallenges [15; 18]. In horizontal FL, the global model\nupdate at a server is an additive aggregation of the local\nmodels, which are updated by each client using its own data.\nIn contrast, the global model in vertical FL is the concatena-\ntion of local models, which are coupled by the loss function,\nso updating a client’s local model requires the information\nof the other clients’ models. Stronger model dependence in\nthe vertical setting leads to challenges on privacy protection\nand communication efﬁciency.\n1.1. Prior art\nWe review prior work from the following three categories.\nFederated learning. Since the seminal work [19; 24], there\nhas been a large body of studies on FL in diverse settings.\nThe most common FL setting is the horizontal setting, where\na large set of data are partitioned among '), Document(metadata={'Published': '2020-07-03', 'Title': 'Privacy Threats Against Federated Matrix Factorization', 'Authors': 'Dashan Gao, Ben Tan, Ce Ju, Vincent W. Zheng, Qiang Yang', 'Summary': 'Matrix Factorization has been very successful in practical recommendation applications and e-commerce. Due to data shortage and stringent regulations, it can be hard to collect sufficient data to build performant recommender systems for a single company. Federated learning provides the possibility to bridge the data silos and build machine learning models without compromising privacy and security. Participants sharing common users or items collaboratively build a model over data from all the participants. There have been some works exploring the application of federated learning to recommender systems and the privacy issues in collaborative filtering systems. However, the privacy threats in federated matrix factorization are not studied. In this paper, we categorize federated matrix factorization into three types based on the partition of feature space and analyze privacy threats against each type of federated matrix factorization model. We also discuss privacy-preserving approaches. As far as we are aware, this is the first study of privacy threats of the matrix factorization method in the federated learning framework.'}, page_content='Privacy Threats Against Federated Matrix Factorization\nDashan Gao1,2∗, Ben Tan3 , Ce Ju3 , Vincent W. Zheng3 and Qiang Yang1,3\n1Department of CSE, Hong Kong University of Science and Technology\n2Department of CSE, Southern University of Science and Technology\n3AI Lab, WeBank Co. Ltd\ndgaoaa@connect.ust.hk, {btan, ceju,vincentz}@webank.com, qyang@cse.ust.hk\nAbstract\nMatrix Factorization has been very successful\nin practical recommendation applications and e-\ncommerce. Due to data shortage and stringent reg-\nulations, it can be hard to collect sufﬁcient data to\nbuild performant recommender systems for a single\ncompany. Federated learning provides the possibil-\nity to bridge the data silos and build machine learn-\ning models without compromising privacy and se-\ncurity. Participants sharing common users or items\ncollaboratively build a model over data from all the\nparticipants. There have been some works explor-\ning the application of federated learning to recom-\nmender systems and the privacy issues in collabora-\ntive ﬁltering systems. However, the privacy threats\nin federated matrix factorization are not studied. In\nthis paper, we categorize federated matrix factor-\nization into three types based on the partition of\nfeature space and analyze privacy threats against\neach type of federated matrix factorization model.\nWe also discuss privacy-preserving approaches. As\nfar as we are aware, this is the ﬁrst study of pri-\nvacy threats of the matrix factorization method in\nthe federated learning framework.\n1\nIntroduction\nRecommender systems play a signiﬁcant role in various ap-\nplications, such as e-commerce and movie recommendation.\nMatrix Factorization (MF) [Koren et al., 2009b], as a typ-\nical Collaborative Filter (CF) method, has positioned itself\nas one of the effective means of generating recommendations\nand is widely adopted in real-world applications. Tradition-\nally, for one company, it is essential to accumulate sufﬁcient\npersonal rating data to build a performant MF model. How-\never, due to the sparse nature of user-item interactions, it\ncan be hard for a single company to collect sufﬁcient data\nto build an MF model. Moreover, recently enacted stringent\nlaws and regulations such as General Data Protection Reg-\nulation (GDPR) [Albrecht, 2016] and California Consumer\nPrivacy Act (CCPA) [Ghosh, 2018] stipulate rules on data\n∗Contact Author\nsharing among companies and organizations, making collab-\noration between companies by sharing personal rating data\nillegal and impractical.\nTo tackle the challenge of protecting individual privacy\nand remitting the data shortage issue, federated learning (FL)\n[Koneˇcn`y et al., 2016; McMahan et al., 2017] provides a\npromising way that enables different parties collaboratively\nbuild a machine learning model without exposing private data\nin each party. It addresses data silos and privacy problems to-\ngether. In FL, data can be partitioned horizontally (example-\npartitioned) or vertically (feature-partitioned) into different\nparties. When records are not aligned between parties and\nthe feature spaces among parties are heterogeneous, feder-\nated transfer learning can be adopted. The use of FL in rec-\nommender systems has been studied over different data distri-\nbutions. For example, [Chai et al., 2019] considers horizon-\ntally partitioned rating data among clients, which hold ratings\nof the same user-item interaction matrix. Federated multi-\nview MF is studied where participants hold item interaction\ndata, item features, or user features [Flanagan et al., 2020].\nEach participant holds a part of model parameters, while\nsome common parameters are shared among participants.\nExisting studies generally categorize horizontal and verti-\ncal federated recommender systems regarding on whether\nuser alignment is required before FL\n[Yang et al., 2019;\nChai et al., 2019]. For example, participants sharing different\nusers and the same set of items implies horizontal federated\nrecommender systems. In our paper, '), Document(metadata={'Published': '2019-10-18', 'Title': 'Federated Generative Privacy', 'Authors': 'Aleksei Triastcyn, Boi Faltings', 'Summary': 'In this paper, we propose FedGP, a framework for privacy-preserving data release in the federated learning setting. We use generative adversarial networks, generator components of which are trained by FedAvg algorithm, to draw privacy-preserving artificial data samples and empirically assess the risk of information disclosure. Our experiments show that FedGP is able to generate labelled data of high quality to successfully train and validate supervised models. Finally, we demonstrate that our approach significantly reduces vulnerability of such models to model inversion attacks.'}, page_content='Federated Generative Privacy\nAleksei Triastcyn , Boi Faltings\nArtiﬁcial Intelligence Lab\nEcole Polytechnique F´ed´erale de Lausanne\nLausanne, Switzerland\n{aleksei.triastcyn, boi.faltings}@epﬂ.ch,\nAbstract\nIn this paper, we propose FedGP, a framework\nfor privacy-preserving data release in the federated\nlearning setting. We use generative adversarial net-\nworks, generator components of which are trained\nby FedAvg algorithm, to draw privacy-preserving\nartiﬁcial data samples and empirically assess the\nrisk of information disclosure.\nOur experiments\nshow that FedGP is able to generate labelled data\nof high quality to successfully train and validate su-\npervised models. Finally, we demonstrate that our\napproach signiﬁcantly reduces vulnerability of such\nmodels to model inversion attacks.\n1\nIntroduction\nThe rise of data analytics and machine learning (ML) presents\ncountless opportunities for companies, governments and in-\ndividuals to beneﬁt from the accumulated data.\nAt the\nsame time, their ability to capture ﬁne levels of detail po-\ntentially compromises privacy of data providers. Recent re-\nsearch [Fredrikson et al., 2015; Shokri et al., 2017; Hitaj et\nal., 2017] suggests that even in a black-box setting it is possi-\nble to argue about the presence of individual examples in the\ntraining set or recover certain features of these examples.\nAmong methods that tackle privacy issues of ma-\nchine learning is the recent concept of federated learning\n(FL) [McMahan et al., 2016]. In the FL setting, a central\nentity (server) wants to train a model on user data without ac-\ntually copying these data from user devices. Instead, users\n(clients) update models locally, and the server aggregates\nthese models. One popular approach is the federated averag-\ning, FedAvg [McMahan et al., 2016], where clients do local\non-device gradient descent using their data, then send these\nupdates to the server where they get averaged. Privacy can\nfurther be enhanced by using secure multi-party computation\n(MPC) [Yao, 1982] to allow the server access only average\nupdates of a big group of users and not individual ones.\nDespite many advantages, federated learning does have a\nnumber of challenges.\nFirst, the result of FL is a single\ntrained model (therefore, we will refer to it as a model re-\nlease method), which does not provide much ﬂexibility in the\nfuture. For instance, it would signiﬁcantly reduce possibili-\nties for further aggregation from different sources, e.g. differ-\nGenerator\nCritic 1\nSensitive Data 1\nLabels\nNoise\nArtiﬁcial Data\nReal\nFake\nML\nCritic 2\nSensitive Data 2\nReal\nFake\nFigure 1: Architecture of our solution for two clients. Sensitive data\nis used to train a GAN (local critic and federated generator) to pro-\nduce a private artiﬁcial dataset, which can be used by any ML model.\nent hospitals trying to combine federated models trained on\ntheir patients data. Second, this solution requires data to be\nlabelled at the source, which is not always possible, because\nuser may be unqualiﬁed to label their data or unwilling to do\nso. A good example is again a medical application where\nusers are unqualiﬁed to diagnose themselves but at the same\ntime would want to keep their condition private. Third, it\ndoes not provide provable privacy guarantees, and there is no\nreason to believe that the aforementioned attacks do not work\nagainst it. Some papers propose to augment FL with differen-\ntial privacy (DP) to alleviate this issue [McMahan et al., 2017;\nGeyer et al., 2017]. While these approaches perform well in\nML tasks and provide theoretical privacy guarantees, they are\noften restrictive (e.g. many DP methods for ML assume, im-\nplicitly or explicitly, access to public data of similar nature or\nabundant amounts of data, which is not always realistic).\nIn our work, we address these problems by propos-\ning to combine the strengths of federated learning and re-\ncent advancements in generative models to perform privacy-\npreserving data release, which has many immediate '), Document(metadata={'Published': '2020-09-08', 'Title': 'A Review of Privacy-preserving Federated Learning for the Internet-of-Things', 'Authors': 'Christopher Briggs, Zhong Fan, Peter Andras', 'Summary': "The Internet-of-Things (IoT) generates vast quantities of data, much of it attributable to individuals' activity and behaviour. Gathering personal data and performing machine learning tasks on this data in a central location presents a significant privacy risk to individuals as well as challenges with communicating this data to the cloud. However, analytics based on machine learning and in particular deep learning benefit greatly from large amounts of data to develop high-performance predictive models. This work reviews federated learning as an approach for performing machine learning on distributed data with the goal of protecting the privacy of user-generated data as well as reducing communication costs associated with data transfer. We survey a wide variety of papers covering communication-efficiency, client heterogeneity and privacy preserving methods that are crucial for federated learning in the context of the IoT. Throughout this review, we identify the strengths and weaknesses of different methods applied to federated learning and finally, we outline future directions for privacy preserving federated learning research, particularly focusing on IoT applications."}, page_content='A Review of Privacy-preserving Federated\nLearning for the Internet-of-Things\nChristopher Briggs, Zhong Fan and Peter Andras\nAbstract The Internet-of-Things (IoT) generates vast quantities of data, much of it\nattributable to individuals’ activity and behaviour. Gathering personal data and per-\nforming machine learning tasks on this data in a central location presents a signiﬁcant\nprivacy risk to individuals as well as challenges with communicating this data to the\ncloud. However, analytics based on machine learning and in particular deep learning\nbeneﬁt greatly from large amounts of data to develop high-performance predictive\nmodels. This work reviews federated learning as an approach for performing machine\nlearning on distributed data with the goal of protecting the privacy of user-generated\ndata as well as reducing communication costs associated with data transfer. We\nsurvey a wide variety of papers covering communication-eﬃciency, client hetero-\ngeneity and privacy preserving methods that are crucial for federated learning in the\ncontext of the IoT. Throughout this review, we identify the strengths and weaknesses\nof diﬀerent methods applied to federated learning and ﬁnally, we outline future di-\nrections for privacy preserving federated learning research, particularly focusing on\nIoT applications.\n1 Introduction\nThe Internet-of-Things (IoT) is represented by network-connected machines, often\nsmall embedded computers that provide physical objects with digital capabilities\nsuch as identiﬁcation, inventory tracking and sensing & actuator control. Mobile\ndevices such as smartphones also represent a facet of the IoT, often used as a sensing\nChristopher Briggs\nKeele University, Staﬀordshire, UK, e-mail: c.briggs@keele.ac.uk\nZhong Fan\nKeele University, Staﬀordshire, UK e-mail: z.fan@keele.ac.uk\nPeter Andras\nKeele University, Staﬀordshire, UK e-mail: p.andras@keele.ac.uk\n1\narXiv:2004.11794v2  [cs.LG]  8 Sep 2020\n2\nChristopher Briggs, Zhong Fan and Peter Andras\ndevice as well as to control and monitor other IoT devices. The applications that\ndrive analytical insights in the IoT are often powered by machine learning and deep\nlearning.\nGartner [1] predicts that 25 billion IoT devices will be in use by 2021, forecasting\na bright future for IoT applications. However this poses a challenge for traditional\ncloud-based IoT computing. The volume, velocity and variety of data streaming from\nbillions of these devices requires vast amounts of bandwidth which can become\nextremely cost prohibitive. Additionally, many IoT applications require very low-\nlatency or near real-time analytics and decision making capabilities. The round-trip\ndelay from devices to the cloud and back again is unacceptable for such applications.\nFinally, transmittingsensitive data collectedby IoTdevices to thecloud poses security\nand privacy concerns. Edge computing, and more recently, fog computing [2] have\nbeen proposed as a solution to these problems.\nEdge computing (and its variants: mobile edge computing, multi-access edge\ncomputing) restrict analytics processing to the edge of the network âĂŞ on devices\nattached to, or very close to the perception layer [3]. However storage and compute\npower may be severely limited and coordination between multiple devices may be\nnon-existent in the edge computing paradigm. Fog computing [2] oﬀers an alternative\nto cloud computing or edge computing alone for many analytics tasks but signiﬁcantly\nincreases the complexity of an IoT network. Fog computing is generally described as\na continuum of compute, storage and networking capabilities to power applications\nand services in one or more tiers that bridge the gap between the cloud and the\nedge [4, 5]. Fog computing enables highly scalable, low-latency, geo-distributed\napplications, supporting location awareness and mobility [6]. Despite rising interest\nin fog-based computing, much research is still focused on deployment of analytics\napplications (including deep learning applications) d')]